% ============================================
% TITLE AND AUTHOR INFORMATION
% ============================================

\title{Hierarchical Action Models with 2D Paths and Consistency Flow Training for General Robot Manipulation}

\author{Naoto Ota}

\date{}

% ============================================
% ABSTRACT
% ============================================

\begin{abstract}
Hierarchical vision-language-action (VLA) models that separate high-level semantic planning from low-level motion generation have emerged as a promising approach for robot manipulation. This paper investigates the integration of ManiFlow, a Consistency Flow Matching-based policy capable of generating high-quality actions in 1-2 inference steps, as the low-level policy within a hierarchical architecture guided by 2D end-effector paths from a vision-language model (VILA). We conduct systematic experiments in the RoboTwin 2.0 simulation environment across 3 manipulation tasks and 6 experimental conditions, examining the effects of path guidance on both in-domain accuracy and cross-domain generalization. Our experiments reveal three key findings. First, models trained on visually clean environments consistently outperform those trained on cluttered environments when evaluated on cluttered scenes (average success rate 20.3-28.0\% vs 9.3-10.3\%), suggesting that learning in simplified visual conditions facilitates acquisition of essential motion patterns and improves generalization. Second, VILA path guidance demonstrates effectiveness in cross-domain settings, achieving +7.7\% average improvement with current path input, indicating that semantic guidance from VLMs can help bridge domain gaps. Third, the Memory Function that combines initial and current paths unexpectedly degrades performance, revealing that additional input information is not always beneficial and requires careful design. These results provide insights into the design of hierarchical robot manipulation systems and highlight both the potential and limitations of VLM-guided path conditioning for generalizable manipulation policies.
\end{abstract}

\textbf{Keywords}: Robot Manipulation, Hierarchical Model, Consistency Flow Matching, Path Guidance, Generalization

% ============================================
% SECTION 1: INTRODUCTION
% ============================================

\section{Introduction}

Developing general-purpose robot manipulation policies remains a long-standing challenge in robotics. Recent advances in large-scale vision-language models (VLMs) have spurred efforts to leverage their rich world knowledge for robot control \cite{ref1, ref2, ref25, ref26}. Vision-Language-Action (VLA) models, which directly fine-tune VLMs to predict robot actions, can generate control signals end-to-end from task descriptions and image observations, showing promise for generalization in open-world environments \cite{ref2, ref27, ref28}.

However, such monolithic VLA models face several challenges. First, collecting high-quality robot manipulation data is costly, limiting the scale and diversity of available training data \cite{ref3, ref29, ref30}. Second, VLA models suffer from inference speed constraints, making them difficult to apply to tasks requiring fast and precise motion control \cite{ref2, ref4}. To address these challenges, hierarchical approaches that separate high-level semantic understanding by VLMs from precise motion generation by smaller policies have been proposed \cite{ref3, ref5, ref11}.

HAMSTER is a hierarchical VLA model that uses a VLM (VILA-1.5-13B) to generate 2D end-effector paths, which serve as guidance for low-level policies such as RVT-2 or 3D Diffuser Actor \cite{ref3}. This design allows the VLM to focus on semantic trajectory planning while the low-level policy concentrates on precise control in 3D space. HAMSTER is also notable for its ability to leverage off-domain data without action labels, such as simulation data or data from different embodiments.

Meanwhile, generative models based on diffusion have demonstrated strong performance for robot manipulation \cite{ref6, ref13, ref31}. However, diffusion models require numerous denoising steps during inference, posing challenges for real-time control. ManiFlow addresses this by adopting Consistency Flow Matching, enabling high-quality action generation in just 1-2 steps \cite{ref7}. ManiFlow employs the DiT-X (Diffusion Transformer with Cross-Attention) architecture to efficiently integrate diverse inputs including visual, language, and proprioceptive information.

In this work, we propose adopting ManiFlow as the low-level policy within HAMSTER's hierarchical architecture and integrating 2D paths generated by the VLM into ManiFlow's input. While the original ManiFlow was trained for single-task learning and evaluated under conditions identical to training, we investigate whether introducing VILA path guidance can improve (1) single-task accuracy in the same environment, and (2) generalization performance to conditions different from training.

Specifically, we propose two patterns for path input. The first pattern uses only the path prediction for the current frame. The second pattern additionally retains the path prediction from episode onset, using it alongside the current path. We refer to the latter as the Memory Function. This design aims to enable reference to stable past path information when current-frame path generation becomes unstable due to occlusion or other factors.

Evaluation experiments are conducted in the RoboTwin 2.0 simulation environment \cite{ref8}. We establish six experimental conditions by combining training environments (clean table or cluttered table) with evaluation environments (cluttered table), and quantitatively analyze the effects of path guidance on single-task accuracy and generalization performance.

% ============================================
% SECTION 2: RELATED WORK
% ============================================

\section{Related Work}

\subsection{Hierarchical Robot Manipulation Policies}

Hierarchical approaches in robot manipulation aim to solve complex tasks by separating high-level planning from low-level control. SayCan proposed combining the knowledge of large language models (LLMs) with robot affordance functions to generate action plans that are both executable and semantically appropriate \cite{ref5}. Code as Policies demonstrated a hierarchical code generation approach that uses LLMs to generate program code for robot control, enabling complex spatial reasoning \cite{ref9}. Inner Monologue achieved closed-loop reasoning by providing environmental feedback to LLMs, integrating success detection and scene descriptions \cite{ref32}.

In hierarchical VLA models using VLMs, the choice of intermediate representation generated by the high-level model is an important design decision. Keypoint-based affordance prediction \cite{ref10} predicts manipulation target positions and placement locations as points, achieved through VLM fine-tuning. RT-Trajectory used 2D trajectory sketches as an intermediate representation to enable flexible task specification and policy generalization \cite{ref11}. For low-level policies, 3D-aware policies such as Perceiver-Actor \cite{ref33} using voxelized 3D observations, and RVT \cite{ref34} and RVT-2 \cite{ref35} using multi-view images from virtual viewpoints, have demonstrated high accuracy.

HAMSTER proposed a hierarchical architecture where a VLM predicts 2D end-effector paths, which serve as guidance for low-level 3D-aware policies \cite{ref3}. A key advantage of this design is that the high-level VLM can be trained on off-domain data without action labels, such as simulation data or data from different embodiments. In HAMSTER, methods of overlaying path information on images and concatenating it as a separate input dimension were compared, with the latter showing superior performance. In this work, we adopt the method of overlaying paths onto images and using them as input to ManiFlow.

\subsection{Diffusion Models and Flow Matching for Robot Control}

Diffusion models are generative models that learn gradual transformations from noise to data, achieving significant success in image generation \cite{ref12, ref36}. Diffusion Policy applied diffusion models to robot visuomotor policies, demonstrating strong performance in representing multimodal action distributions and handling high-dimensional action spaces \cite{ref6}. 3D Diffusion Policy further improved visual generalization by using 3D representations extracted from sparse point clouds \cite{ref13}. 3D Diffuser Actor combined 3D scene representations with Diffusion Policy, achieving high performance on the RLBench benchmark \cite{ref31}. However, diffusion models require numerous denoising steps during inference (typically 10-100 steps), posing challenges for real-time control applications.

Several approaches have been proposed to address this inference efficiency problem. DDIM, as a deterministic sampling variant of DDPM, enabled high-quality generation with fewer steps \cite{ref37}. Consistency Models enabled single-step generation by learning direct mappings from arbitrary points in the diffusion process to target data \cite{ref14}. For robot control applications, Consistency Policy proposed accelerating inference through knowledge distillation from pretrained Diffusion Policies \cite{ref19}. ManiCM applied Consistency Models to 3D Diffusion Policy, achieving real-time 3D manipulation \cite{ref20}.

Flow Matching, on the other hand, constructs generative models through a different approach than diffusion models. Flow Matching directly learns continuous transformations (flows) from noise distributions to data distributions in a simulation-free manner \cite{ref15}. Rectified Flow achieved more efficient sampling by straightening these flows \cite{ref21}. These methods offer more stable training and higher-quality generation with fewer steps compared to diffusion models. $\pi_0$ combined VLA models with Flow Matching, achieving general control across diverse robot configurations including single-arm, bimanual, and mobile manipulators \cite{ref22}.

ManiFlow adopts Consistency Flow Matching, which integrates Flow Matching and Consistency Training \cite{ref7}. Specifically, in addition to the standard Flow Matching loss, it simultaneously optimizes a continuous-time Consistency loss that constrains different points on flow trajectories to converge to the same target data point. This design enables high-quality generation in 1-2 steps without requiring pretrained teacher models. The core of ManiFlow's architecture, DiT-X, is built upon Diffusion Transformer \cite{ref16} and efficiently integrates diverse input modalities including visual, language, and proprioceptive information through cross-attention and AdaLN-Zero conditioning. In this work, we adopt ManiFlow as the low-level policy and integrate VLM-generated paths as additional visual input.

\subsection{Domain Adaptation and Generalization}

The domain gap between training and evaluation environments is a critical challenge when deploying robot manipulation policies to real environments. Domain Randomization, which randomizes rendering parameters in simulation environments, is widely used to facilitate transfer to real environments \cite{ref17}. Colosseum is a benchmark that evaluates policy generalization through 20 tasks $\times$ 14 axes of environmental perturbations, measuring robustness to changes in color, texture, size, lighting, and background \cite{ref38}.

Pretraining visual representations is also an effective approach for improving generalization. R3M pretrained visual representations using large-scale human video data, improving transfer performance to robot manipulation tasks through temporal contrastive learning and video-language alignment \cite{ref18}. CLIP performed contrastive learning on 400 million image-text pairs, achieving zero-shot transfer capabilities \cite{ref39}. DINOv2 performed self-supervised learning on 142 million images, providing general visual features without fine-tuning \cite{ref40}.

RoboTwin 2.0 is a large-scale benchmark containing 731 objects, 50 tasks, and 5 robot embodiment types, implementing 5 axes of Domain Randomization including Clutter, Lighting, and Background \cite{ref8}. Compared to standard benchmarks such as RLBench \cite{ref41} and MetaWorld \cite{ref42}, it enables evaluation with more realistic visual variations. In this work, we use the RoboTwin 2.0 environment to evaluate how well policies trained on clean table conditions can generalize to cluttered table conditions. Whether VLM path guidance can bridge such domain gaps is one of the main questions investigated in this study.

% ============================================
% SECTION 3: PROPOSED METHOD
% ============================================

\section{Proposed Method}

This chapter describes the proposed system that integrates 2D path guidance from VLMs into ManiFlow.

\subsection{System Overview}

The proposed system adopts a hierarchical architecture consisting of a high-level path generation module and a low-level action generation module. Figure 1 shows the overall system configuration.

% [Figure 1: System overview - VILA-13B (high-level) on the left, ManiFlow (low-level) on the right. RGB image and task instruction as input at the top, 2D path output from VILA overlaid onto RGB image and fed to ManiFlow. ManiFlow outputs 16 steps of actions. Both initial overlay and current overlay paths are illustrated.]

The high-level module uses VILA-1.5-13B \cite{ref23}. VILA takes an RGB image and task description as input and outputs the 2D path that the end-effector should follow. The output path is represented as a sequence of normalized 2D coordinates (each coordinate in the range [0, 1]) and gripper states (open/close). We use the model fine-tuned on robot manipulation data from the HAMSTER project \cite{ref3}.

The low-level module uses ManiFlow \cite{ref7}. The original ManiFlow takes RGB images from the past 2 steps and the robot's proprioceptive state (joint angles and gripper state) as input, generating 16 steps of actions at once. In this work, we overlay the 2D path generated by VILA onto these input images, thereby conveying high-level trajectory guidance to the low-level policy.

The proposed system provides two patterns for path input:

\textbf{Pattern 1: Current path only}

Only the path generated by VILA for the current frame is used. Specifically, the image with the path overlaid on the current observation (current overlay) and the robot state are provided as input to ManiFlow.

\textbf{Pattern 2: Initial path + Current path (Memory Function)}

Both the path generated at episode onset (initial path) and the path generated for the current frame (current path) are used. This design aims to enable reference to stable initial path information when current-frame path generation becomes unstable due to occlusion during task execution. We refer to this mechanism as the Memory Function.

The inference flow proceeds as follows. Since ManiFlow predicts 16 steps of actions at once, path generation occurs every 16 steps rather than every frame:

\begin{enumerate}
    \item \textbf{Step 0}: Execute VILA path generation and create initial overlay and current overlay. Input these to ManiFlow and predict 16 steps of actions at once.
    \item \textbf{Steps 1-15}: Sequentially execute the predicted actions.
    \item \textbf{Step 16}: Execute new VILA path generation and update the current overlay. The initial overlay remains fixed from episode onset. Predict the next 16 steps of actions at once.
    \item Repeat every 16 steps until task completion.
\end{enumerate}

This design limits the overhead of VILA path generation to once every 16 steps, minimizing the impact on real-time control.

\subsection{High-Level Path Generation}

The high-level module uses VILA-1.5-13B \cite{ref23}, which was fine-tuned on robot manipulation data in the HAMSTER project \cite{ref3}. VILA is a Vision-Language Model combining the SigLIP visual encoder with the Vicuna language model, pretrained on 5.3 billion image-text pairs.

\subsubsection{Input and Output}

The input to VILA consists of an RGB image and a natural language task instruction. Task instructions are given in forms such as ``Pick up the hammer and beat the block'' or ``click the bell's top center on the table.''

VILA's output is structured in the following format:

\begin{verbatim}
<ans>[(x1, y1), (x2, y2), <action>Close Gripper</action>, (x3, y3), ...]</ans>
\end{verbatim}

Here, each coordinate (x, y) represents a 2D position normalized to the range [0, 1], and \texttt{<action>} tags indicate gripper state change points. By parsing this output, we obtain a path representation including the gripper state (open/close) corresponding to each waypoint.

\subsubsection{Path Visualization}

Generated paths are rendered as overlays on the original RGB image. The rendering specifications follow the original HAMSTER \cite{ref3} implementation:

\begin{itemize}
    \item \textbf{Path color}: Uses the jet colormap, encoding temporal progression through color changes from blue $\rightarrow$ cyan $\rightarrow$ green $\rightarrow$ yellow $\rightarrow$ red
    \item \textbf{Line width}: Scaled according to image size (based on 512$\times$512 images)
    \item \textbf{Gripper markers}: Circular markers drawn only at gripper state change points
    \begin{itemize}
        \item Open state: Blue circle (BGR: 255, 0, 0)
        \item Close state: Red circle (BGR: 0, 0, 255)
    \end{itemize}
\end{itemize}

Figure 2 shows examples of path overlays.

% [Figure 2: Path overlay examples - For three tasks (click\_bell, move\_can\_pot, beat\_block\_hammer), overlay images for both clean and cluttered conditions are displayed side by side. Examples showing path color changes via jet colormap and markers at gripper state change points.]

\subsubsection{Path Generation Reliability}

Since VILA is configured with temperature=0, the output is deterministic, but some frames may produce outputs that do not follow the specified format. To address this issue, we adopt the following strategies:

\begin{enumerate}
    \item \textbf{Retry mechanism}: Up to 2 path generation attempts per frame
    \item \textbf{Fallback parsing}: Automatic completion for outputs interpretable as coordinate data even when \texttt{<ans>} tags are omitted or coordinate parentheses are missing
    \item \textbf{Fallback path}: When path generation fails, the most recently successful path is used as a substitute. For consecutive failures at episode onset, the first successful path within the episode is retroactively applied
\end{enumerate}

This design ensures the same fallback strategy is applied during both training and evaluation, maintaining consistency in the input distribution.

\subsection{Low-Level Action Generation}

The low-level module uses ManiFlow \cite{ref7}, a robot manipulation policy based on Consistency Flow Matching. This section describes ManiFlow's architecture and how path input is integrated in this work.

\subsubsection{ManiFlow Overview}

ManiFlow achieves high-quality action generation in 1-2 steps through Consistency Flow Matching, which integrates Flow Matching \cite{ref15} and Consistency Training \cite{ref14}. While conventional Diffusion Policy \cite{ref6} requires 10-100 denoising steps during inference, ManiFlow achieves comparable or better quality with significantly fewer steps.

ManiFlow training simultaneously optimizes the standard Flow Matching loss and Consistency loss. The Flow Matching loss learns the velocity field from noise points to data points:

\begin{equation}
\mathcal{L}_{FM}(\theta) = \mathbb{E}_{x_0, x_1}[\|v_\theta(x_t, t) - (x_1 - x_0)\|^2]
\end{equation}

Here, $x_t = (1-t)x_0 + tx_1$ is the linear interpolation between noise $x_0$ and data $x_1$. The Consistency loss constrains different points on flow trajectories to converge to the same target:

\begin{equation}
\mathcal{L}_{CT}(\theta) = \mathbb{E}_{t, \Delta t}[\|v_\theta(x_t, t, \Delta t) - \tilde{v}_{target}\|^2]
\end{equation}

By combining these losses, high-quality generation is achieved in few steps without requiring pretrained teacher models.

\subsubsection{DiT-X Architecture}

The core of ManiFlow's architecture is DiT-X (Diffusion Transformer with Cross-Attention). DiT-X is built upon Diffusion Transformer \cite{ref16} and features:

\begin{itemize}
    \item \textbf{Adaptive cross-attention}: Enables fine-grained token-level interactions between action tokens and observation features (visual, language)
    \item \textbf{AdaLN-Zero conditioning}: Dynamically generates scale and shift parameters based on low-dimensional inputs such as timesteps and robot states, adaptively adjusting network behavior
    \item \textbf{Multi-modal integration}: Efficiently integrates visual input, language input, and proprioceptive input
\end{itemize}

\subsubsection{Path Input Integration in This Work}

The original ManiFlow takes RGB images from the past 2 steps and the robot's proprioceptive state (14 dimensions: 7 DoF joint positions and gripper state for each arm) as input, generating 16 steps of actions at once.

In this work, we integrate high-level guidance from VILA by replacing the input RGB images with path overlay images. Table~\ref{tab:input_config} shows the specific input configuration for each condition.

\begin{table}[h]
\centering
\caption{Input Configuration for Each Condition}
\label{tab:input_config}
\begin{tabular}{lccc}
\hline
Condition & Input Image & Proprioception & Output \\
\hline
Original ManiFlow & RGB image (past 2 steps) & 14 dim & 16 steps of actions \\
Current path only & current overlay (past 2 steps) & 14 dim & 16 steps of actions \\
Initial + Current & initial overlay + current overlay & 14 dim & 16 steps of actions \\
\hline
\end{tabular}
\end{table}

Path overlay images are the original RGB images with the 2D path generated by VILA visually rendered on them. This design allows adding high-level trajectory guidance as input without modifying ManiFlow's architecture itself. ManiFlow is expected to learn the overlaid path information as visual features and utilize it for task execution.

\subsection{Memory Function}

In this work, we propose a second pattern for path input that combines the path from episode onset (initial path) with the current frame path (current path). We refer to this mechanism as the Memory Function.

\subsubsection{Design Motivation}

During task execution, occlusion may occur when the robot's end-effector or grasped objects obstruct the field of view. In such situations, the quality of paths generated by VILA may degrade, producing inaccurate or incomplete trajectories.

At episode onset (frame 0), occlusion typically does not occur, and the task target objects are clearly visible. Therefore, paths generated at this point are considered reliable as overall trajectory plans for the task. The Memory Function maintains this initial path, enabling reference to overall task context information even when current path quality degrades during task execution.

\subsubsection{Input Configuration}

When using the Memory Function, input to ManiFlow is configured as follows:

\begin{itemize}
    \item \textbf{Initial overlay}: RGB image at episode onset (frame 0) with the path generated at that time overlaid. Fixed until episode end.
    \item \textbf{Current overlay}: Current frame RGB image with the path generated for that frame overlaid. Updated every 16 steps.
    \item \textbf{Proprioceptive state}: Current robot state (14 dimensions)
\end{itemize}

Figure 3 shows a conceptual diagram of the Memory Function.

% [Figure 3: Memory Function conceptual diagram - Along the time axis, illustrating how the initial overlay is generated and fixed at frame 0, and how the current overlay is updated at frames 16, 32, etc. Visually expressing that even when current path becomes unstable due to occlusion, the initial path remains available for reference.]

\subsubsection{Expected Effects}

The Memory Function is expected to provide the following benefits:

\begin{enumerate}
    \item \textbf{Improved occlusion robustness}: Even when path generation becomes unstable in the current frame, the initial path provides trajectory information for the overall task
    \item \textbf{Task context preservation}: The initial path contains the complete trajectory plan at task onset, serving as a cue for grasping the overall task picture
    \item \textbf{Utilization of long-term planning information}: While ManiFlow predicts 16 steps of actions, reference to the initial path may enable motion generation based on longer-term planning
\end{enumerate}

\subsubsection{Comparison with Current Path Only}

Table~\ref{tab:path_comparison} shows the differences between using only the current path (Pattern 1) and using the Memory Function (Pattern 2).

\begin{table}[h]
\centering
\caption{Comparison of Path Input Patterns}
\label{tab:path_comparison}
\begin{tabular}{lcc}
\hline
Item & Current path only & Memory Function \\
\hline
Number of input images & 2 (past 2 steps) & 3 (initial + past 2 steps) \\
Path update frequency & Every 16 steps & current: every 16 steps / initial: fixed \\
Occlusion robustness & Low & High (initial path available for reference) \\
Overall task context & Only current local information & Maintains overall plan from initial state \\
Computational cost & Low & Slightly higher (one additional input image) \\
\hline
\end{tabular}
\end{table}

In our experiments, we compare these two patterns and quantitatively verify the effectiveness of the Memory Function.

% ============================================
% SECTION 4: EXPERIMENTS
% ============================================

\section{Experiments}

This chapter describes the experimental setup and results for verifying the effectiveness of the proposed method.

\subsection{Experimental Setup}

Experiments were conducted in the RoboTwin 2.0 simulation environment \cite{ref8}. RoboTwin 2.0 is built on the SAPIEN 3.0.0b1 physics engine \cite{ref24} and provides benchmarks for bimanual manipulation tasks using the AgileX Cobot Magic robot. This work focuses on single-arm tasks, using the clean table and cluttered table conditions provided as table environment settings. In the cluttered table condition, objects unrelated to the task are randomly placed on the table, increasing visual complexity.

For evaluation tasks, we selected 3 tasks from RoboTwin 2.0's single-arm tasks (35 tasks), considering difficulty and diversity of motion patterns. Click\_bell involves simple contact motion, move\_can\_pot is a standard pick-and-place task, and beat\_block\_hammer requires tool use with two-stage motion. For each task, we collected 50 successful trajectories using RoboTwin 2.0's scripted expert policy as training data.

In this work, we established 6 experimental conditions (C1-C6) by combining 3 model configurations (original ManiFlow, VILA + ManiFlow (current path), VILA + ManiFlow (initial + current path)) with 2 training environments (cluttered, clean). All conditions were evaluated on the cluttered table condition. Conditions C1-C3 are in-domain settings with training and evaluation on cluttered environments, while conditions C4-C6 are cross-domain settings with training on clean environments and evaluation on cluttered environments. This design tests the following four hypotheses: \textbf{H1} (VILA path guidance improves in-domain accuracy), \textbf{H2} (VILA path guidance improves cross-domain generalization), \textbf{H3} (Memory Function outperforms current path only), and \textbf{H4} (path guidance effects are more pronounced in cross-domain conditions).

For evaluation metrics, we used task success rate based on 100 trials per task as the primary metric, with success determined using RoboTwin 2.0's built-in judgment engine. As auxiliary metrics, we measured VILA path generation time, ManiFlow action generation time, and path generation success rate. ManiFlow training settings were unified across all conditions: 2 observation steps, 16 action prediction steps, and 501 training epochs.

\subsection{Results and Analysis}

\subsubsection{Effect of Training Environment}

% [Figure: Success rate by condition]

Table~\ref{tab:success_rate} shows success rates across 3 tasks $\times$ 6 conditions. From these results, we verify \textbf{H1} (in-domain path guidance effect) and \textbf{H4} (generalization capability of clean environment training).

\begin{table}[h]
\centering
\caption{Task Success Rate (\%)}
\label{tab:success_rate}
\begin{tabular}{lcccccc}
\hline
Task & C1 & C2 & C3 & C4 & C5 & C6 \\
\hline
Click Bell & 4.0 & 2.0 & 4.0 & 3.0 & \textbf{20.0} & 1.0 \\
Move Can to Pot & 21.0 & 20.0 & 26.0 & 27.0 & 27.0 & 27.0 \\
Beat Block with Hammer & 4.0 & 6.0 & 1.0 & 31.0 & \textbf{37.0} & 11.0 \\
\textbf{Average} & 9.7 & 9.3 & 10.3 & 20.3 & \textbf{28.0} & 13.0 \\
\hline
\end{tabular}
\end{table}

\textit{C1-C3: trained on cluttered, C4-C6: trained on clean. All evaluated on cluttered.}

\textit{C1,C4: Original ManiFlow. C2,C5: VILA current path. C3,C6: VILA initial+current path.}

The most striking finding is that conditions trained on clean environments (C4-C6, average 13.0-28.0\%) consistently showed higher success rates than conditions trained on cluttered environments (C1-C3, average 9.3-10.3\%). This result is contrary to initial expectations and \textbf{strongly supports H4}. Particularly for the Beat Block with Hammer task, C4 (31.0\%) and C5 (37.0\%) substantially outperformed C1-C3 (1.0-6.0\%).

This result suggests that models trained on cluttered environments may have overfit to the scatter patterns specific to training data and failed to handle different scatter patterns during evaluation. In contrast, models trained on clean environments could efficiently learn essential motion patterns for tasks in environments with less visual noise, resulting in improved generalization to unseen cluttered environments.

% [Figure: Training curve comparison]

To support this interpretation, we analyzed training curves. As shown in Figure X, training loss converged sufficiently across all 18 conditions (99.9\% loss reduction), indicating that training completed normally. However, we observed that initial loss for clean environments (C4-C6) tended to be lower than for cluttered environments (C1-C3). Particularly for Beat Block with Hammer, clean environment initial loss (2.25-2.28) was approximately 10\% lower than cluttered environments (2.51-2.53). The fact that large evaluation performance differences emerged despite final losses being comparable across all conditions (0.002-0.004) suggests that \textbf{lower training loss does not necessarily indicate higher generalization performance}.

\subsubsection{Effect of Input Representation Mode}

% [Figure: Hypothesis verification results]

To verify \textbf{H1} and \textbf{H2}, we compared baselines (C1, C4) with path guidance conditions (C2, C3, C5, C6).

\textbf{In-domain conditions (H1 verification)}: The change from C1 to C2 was -0.3\% on average, and from C1 to C3 was +0.7\% on average, showing no consistent improvement from VILA path guidance. Therefore, \textbf{H1 was not supported}.

\textbf{Cross-domain conditions (H2 verification)}: The change from C4 to C5 showed an average improvement of +7.7\%, with improvements confirmed in 2 of 3 tasks (Click Bell: +17.0\%, Beat Block with Hammer: +6.0\%). In contrast, the change from C4 to C6 (initial+current) showed a -7.3\% decrease in performance. Therefore, \textbf{H2 was partially supported for the current path mode}.

To verify \textbf{H3}, we compared current path only (C2, C5) with initial+current (C3, C6). In in-domain conditions (C2$\rightarrow$C3), there was a slight improvement of +1.0\% on average, but in cross-domain conditions (C5$\rightarrow$C6), a substantial performance decrease of -15.0\% on average was observed. This result indicates that \textbf{H3 was not supported}.

% [Figure: Memory Function effect comparison]

Regarding the performance degradation of the initial+current approach, the following factors may be considered. First, inputting two overlay images (initial + current) may have increased the amount of information the model needed to process, making learning more difficult. Second, as the episode progressed, information from the initial path (at frame 0) may have diverged from the current situation, providing misleading guidance. In cross-domain conditions in particular, the initial path generated in clean environments is likely to be inconsistent with cluttered evaluation environments.

\subsubsection{Task-Specific Analysis}

Different result patterns were observed across tasks. For Click Bell, substantial improvement was seen in C5 (3.0\%$\rightarrow$20.0\%), while other conditions remained low at 1.0-4.0\%. For Move Can to Pot, differences between conditions were relatively small (20.0-27.0\%), indicating limited effectiveness of path guidance. For Beat Block with Hammer, high success rates were achieved in C4 (31.0\%) and C5 (37.0\%), but C3 (1.0\%) and C6 (11.0\%) showed poor performance.

These results suggest that the effectiveness of VILA path guidance depends on task characteristics. For tasks requiring clear trajectory planning (Click Bell, Beat Block with Hammer), appropriately generated paths contribute to improved motion accuracy. In contrast, for standard pick-and-place tasks like Move Can to Pot, motion patterns are relatively simple, limiting additional improvement from path guidance.

\subsubsection{Training Dynamics}

% [Figure: Training curve details]

We conducted 501 epochs of training for all 18 conditions (3 tasks $\times$ 6 conditions) and analyzed training curves. Table~\ref{tab:training_stats} shows training statistics.

\begin{table}[h]
\centering
\caption{Training Statistics}
\label{tab:training_stats}
\begin{tabular}{llccc}
\hline
Task & Condition & Initial Loss & Final Loss & Min Loss \\
\hline
Click Bell & C1-C3 & 2.76-2.77 & 0.003-0.003 & 0.003-0.003 \\
 & C4-C6 & 2.76-2.77 & 0.003-0.003 & 0.002-0.003 \\
Move Can to Pot & C1-C3 & 2.33-2.34 & 0.002-0.002 & 0.002-0.002 \\
 & C4-C6 & 2.33-2.34 & 0.002-0.003 & 0.002-0.002 \\
Beat Block with Hammer & C1-C3 & 2.51-2.53 & 0.001-0.004 & 0.0003-0.001 \\
 & C4-C6 & 2.25-2.28 & 0.002-0.002 & 0.001-0.001 \\
\hline
\end{tabular}
\end{table}

All conditions achieved 99.9\% loss reduction, and training converged normally. Differences in initial loss values were observed across tasks, with Click Bell (approximately 2.76) $>$ Beat Block with Hammer (approximately 2.25-2.53) $>$ Move Can to Pot (approximately 2.33). For Beat Block with Hammer, clean environment conditions had approximately 10\% lower initial loss than cluttered environment conditions, suggesting that data from clean environments has representations that are easier to learn.

\subsubsection{Computational Cost}

\begin{table}[h]
\centering
\caption{Inference Time (ms)}
\label{tab:inference_time}
\begin{tabular}{cccc}
\hline
Condition & VILA & ManiFlow & Total Overhead \\
\hline
C1, C4 & - & 108-136 & - \\
C2, C5 & 4313-4703 & 118-136 & +4.3-4.7s \\
C3, C6 & 4361-4363 & 118-126 & +4.4s \\
\hline
\end{tabular}
\end{table}

Path generation by VILA incurs an overhead of approximately 4.3-4.7 seconds. Since ManiFlow alone has fast inference time of approximately 100-140ms, with the current design of VILA inference every 16 steps, overall episode execution time is approximately doubled. Considering C5's improvement (+7.7\%) in cross-domain conditions, this overhead may be acceptable in application scenarios where generalization performance is important. For applications requiring real-time performance, lighter VLMs or optimization of path generation frequency is necessary.

\subsection{Limitations}

This study has several limitations.

First, evaluations in this study were conducted only in the RoboTwin 2.0 simulation environment, without verification on real robots. A sim-to-real gap exists between simulation and real environments, and differences in visual appearance, physical properties, and sensor noise may affect performance. The extent to which VILA path guidance is robust to this sim-to-real gap needs to be verified through future real robot experiments.

Second, the tasks used for evaluation were limited to 3 types (click\_bell, move\_can\_pot, beat\_block\_hammer). While these tasks were selected considering difficulty and diversity of motion patterns, the effects of path guidance on more complex long-horizon tasks, bimanual coordination tasks, or contact-rich manipulation tasks remain unverified. Given that the effectiveness of path guidance was suggested to depend on task characteristics, evaluation on a broader set of tasks is necessary.

Third, regarding the unexpected performance degradation caused by the Memory Function (initial + current path), this study did not sufficiently elucidate its causes. The original design intent was to improve robustness by referencing the initial path when occlusion occurs. However, we did not quantitatively evaluate how much occlusion occurred in the tasks used in this experiment, nor how much it affected path generation quality. Future work should quantitatively analyze the frequency and impact of occlusion, and design more sophisticated Memory mechanisms such as adaptive path reference based on occlusion detection.

% ============================================
% SECTION 5: CONCLUSION
% ============================================

\section{Conclusion}

In this work, we proposed a method that adopts ManiFlow as the low-level policy in HAMSTER's hierarchical architecture and integrates 2D paths generated by VLMs into ManiFlow's input. Through systematic experiments with 3 tasks $\times$ 6 conditions in the RoboTwin 2.0 simulation environment, we verified the effects of path guidance on single-task accuracy and generalization performance.

The experiments yielded the following findings. First, models trained on clean environments consistently showed higher generalization performance than models trained on cluttered environments (average success rate 20.3-28.0\% vs 9.3-10.3\%). This suggests that learning in visually simple environments promotes acquisition of essential motion patterns for tasks, consequently improving generalization to unseen environments. Second, VILA path guidance was effective in cross-domain conditions (clean training $\rightarrow$ cluttered evaluation), achieving an average +7.7\% improvement in current path mode. This indicates the potential for VLM semantic guidance to bridge domain gaps. Third, the Memory Function (initial + current path) unexpectedly caused performance degradation, revealing that additional information is not necessarily beneficial in input representation design.

This study has clarified the potential and challenges of hierarchical approaches combining high-level trajectory planning by VLMs with fast action generation by Consistency Flow Matching. Future work should further verify the practicality of the proposed method through real robot verification, evaluation on a broader set of tasks, and design of Memory mechanisms adaptive to occlusion.

% ============================================
% ACKNOWLEDGMENTS
% ============================================

\section*{Acknowledgments}

This work was supported by Cross-Pacific AI Initiative (X-PAI).

% ============================================
% REFERENCES
% ============================================

\begin{thebibliography}{99}

\bibitem{ref1} A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, ``RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2023.

\bibitem{ref2} M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn, ``OpenVLA: An Open-Source Vision-Language-Action Model,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2024.

\bibitem{ref3} Y. Li, Y. Deng, J. Zhang, J. Jang, M. Memmel, R. Yu, C. Garrett, F. Ramos, D. Fox, A. Li, A. Gupta, and A. Goyal, ``HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation,'' \textit{arXiv preprint arXiv:2502.05485}, 2025.

\bibitem{ref4} A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, ``RT-1: Robotics Transformer for Real-World Control at Scale,'' \textit{arXiv preprint arXiv:2212.06817}, 2022.

\bibitem{ref5} M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reber, C. Samiento, N. Siebers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng, ``Do As I Can, Not As I Say: Grounding Language in Robotic Affordances,'' \textit{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{ref6} C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song, ``Diffusion Policy: Visuomotor Policy Learning via Action Diffusion,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2023.

\bibitem{ref7} G. Yan, J. Zhu, Y. Deng, S. Yang, R. Qiu, X. Cheng, M. Memmel, R. Krishna, A. Goyal, X. Wang, and D. Fox, ``ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training,'' \textit{arXiv preprint arXiv:2509.01819}, 2025.

\bibitem{ref8} T. Chen, J. Wang, Y. Mu, Z. Liu, Y. Yuan, Y. Zhang, C. Wang, R. Qiu, P. Lu, and H. Dong, ``RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation,'' \textit{arXiv preprint arXiv:2506.18088}, 2025.

\bibitem{ref9} J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, ``Code as Policies: Language Model Programs for Embodied Control,'' in \textit{Proc. IEEE International Conference on Robotics and Automation (ICRA)}, 2023.

\bibitem{ref10} W. Yuan, T. Ren, M. Memmel, D. Fox, A. Gupta, and A. Goyal, ``RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2024.

\bibitem{ref11} H. Gu, Y. Su, Y. Liu, S. Jiang, K. Pertsch, J. Luo, A. Mandlekar, D. Xu, L. Fei-Fei, and J. Wu, ``RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches,'' \textit{arXiv preprint arXiv:2311.01977}, 2023.

\bibitem{ref12} J. Ho, A. Jain, and P. Abbeel, ``Denoising Diffusion Probabilistic Models,'' in \textit{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{ref13} Y. Ze, G. Zhang, K. Zhang, C. Hu, J. Wang, and H. Dong, ``3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2024.

\bibitem{ref14} Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, ``Consistency Models,'' in \textit{Proc. International Conference on Machine Learning (ICML)}, 2023.

\bibitem{ref15} Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, ``Flow Matching for Generative Modeling,'' in \textit{Proc. International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{ref16} W. Peebles and S. Xie, ``Scalable Diffusion Models with Transformers,'' in \textit{Proc. IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023.

\bibitem{ref17} J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, ``Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World,'' in \textit{Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 2017.

\bibitem{ref18} S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, ``R3M: A Universal Visual Representation for Robot Manipulation,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2022.

\bibitem{ref19} P. Prasad, R. Hoque, S. Tung, I. Pinto, K. Kawaguchi, and Y. Shkurti, ``Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2024.

\bibitem{ref20} Y. Lu, H. Chen, Y. Huang, Z. Chen, X. Cheng, and H. Wang, ``ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation,'' \textit{arXiv preprint arXiv:2406.01586}, 2024.

\bibitem{ref21} X. Liu, C. Gong, and Q. Liu, ``Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow,'' in \textit{Proc. International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{ref22} K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, S. Jakber, T. Kelestemur, S. Levine, A. Lii, I. Liang, H. Luo, S. Nair, K. Pertsch, L. Qi, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, K. Tran, T. Vuong, F. Xia, Z. Xu, T. Xiao, H. Xu, M. Xu, and S. Yeola, ``$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control,'' \textit{arXiv preprint arXiv:2410.24164}, 2024.

\bibitem{ref23} J. Lin, H. Yin, W. Ping, P. Molchanov, M. Shoeybi, and S. Han, ``VILA: On Pre-training for Visual Language Models,'' in \textit{Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024.

\bibitem{ref24} F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su, ``SAPIEN: A SimulAted Part-based Interactive ENvironment,'' in \textit{Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem{ref25} D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Tober, G. Welker, P. Wohlhart, J. Wu, and P. R. Florence, ``PaLM-E: An Embodied Multimodal Language Model,'' in \textit{Proc. International Conference on Machine Learning (ICML)}, 2023.

\bibitem{ref26} D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, J. Luo, Y. L. Tan, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine, ``Octo: An Open-Source Generalist Robot Policy,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2024.

\bibitem{ref27} Open X-Embodiment Collaboration, ``Open X-Embodiment: Robotic Learning Datasets and RT-X Models,'' in \textit{Proc. IEEE International Conference on Robotics and Automation (ICRA)}, 2024.

\bibitem{ref28} X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu, H. Li, and P. Luo, ``Vision-Language Foundation Models as Effective Robot Imitators,'' \textit{arXiv preprint arXiv:2311.01378}, 2023.

\bibitem{ref29} A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. David, K. Black, A. Kumar, C. Xu, Q. Vuong, and C. Finn, ``DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2024.

\bibitem{ref30} H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine, ``BridgeData V2: A Dataset for Robot Learning at Scale,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2023.

\bibitem{ref31} T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, ``3D Diffuser Actor: Policy Diffusion with 3D Scene Representations,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2024.

\bibitem{ref32} W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, ``Inner Monologue: Embodied Reasoning through Planning with Language Models,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2022.

\bibitem{ref33} M. Shridhar, L. Manuelli, and D. Fox, ``Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2022.

\bibitem{ref34} A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox, ``RVT: Robotic View Transformer for 3D Object Manipulation,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2023.

\bibitem{ref35} A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, and D. Fox, ``RVT-2: Learning Precise Manipulation from Few Demonstrations,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2024.

\bibitem{ref36} Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, ``Score-Based Generative Modeling through Stochastic Differential Equations,'' in \textit{Proc. International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{ref37} J. Song, C. Meng, and S. Ermon, ``Denoising Diffusion Implicit Models,'' in \textit{Proc. International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{ref38} A. Pumacay, I. Singh, J. Duan, F. Xia, J. Thomason, and D. Fox, ``THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation,'' in \textit{Proc. Robotics: Science and Systems (RSS)}, 2024.

\bibitem{ref39} A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, ``Learning Transferable Visual Models From Natural Language Supervision,'' in \textit{Proc. International Conference on Machine Learning (ICML)}, 2021.

\bibitem{ref40} M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. J\'{e}gou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, ``DINOv2: Learning Robust Visual Features without Supervision,'' \textit{arXiv preprint arXiv:2304.07193}, 2023.

\bibitem{ref41} S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, ``RLBench: The Robot Learning Benchmark,'' \textit{IEEE Robotics and Automation Letters}, vol. 5, no. 2, pp. 3019-3026, 2020.

\bibitem{ref42} T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, ``Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning,'' in \textit{Proc. Conference on Robot Learning (CoRL)}, 2020.

\end{thebibliography}
