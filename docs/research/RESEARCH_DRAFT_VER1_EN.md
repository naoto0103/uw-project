# Hierarchical Action Models with 2D Paths and Consistency Flow Training for General Robot Manipulation

（2Dパス表現と一貫性フロー学習を用いた階層型生成モデルによる汎用的なロボット操作）

**著者**: 太田尚都

**指導教員**: 田中文英, 宇津呂武仁, 山口友之

---

## Abstract

Hierarchical vision-language-action (VLA) models that separate high-level semantic planning from low-level motion generation have emerged as a promising approach for robot manipulation. This paper integrates ManiFlow, a Consistency Flow Matching-based policy enabling high-quality action generation in 1-2 inference steps, as the low-level policy guided by 2D end-effector paths from a vision-language model (VLM) fine-tuned on robot path data. Through systematic experiments in RoboTwin 2.0 across 3 tasks and 6 conditions, we reveal three key findings. First, models trained on clean environments consistently outperform those trained on cluttered environments when evaluated on cluttered scenes (average success rate 20.3-28.0% vs 9.3-10.3%), suggesting that simplified training conditions facilitate learning essential motion patterns. Second, VLM path guidance improves performance in cross-domain settings (+7.7% average), indicating that semantic guidance can bridge domain gaps. Third, the Memory Function combining initial and current paths unexpectedly degrades performance, revealing that additional input information requires careful design. These results highlight both the potential and limitations of VLM-guided path conditioning for generalizable manipulation policies.

**Keywords**: Robot Manipulation, Hierarchical Model, Consistency Flow Matching, Path Guidance, Generalization

---

## 1. Introduction

Developing general-purpose robot manipulation policies remains a long-standing challenge in robotics. Recent advances in large-scale vision-language models (VLMs) have spurred efforts to leverage their rich world knowledge for robot control [1, 2, 25, 26]. Vision-language-action (VLA) models, which directly fine-tune VLMs to predict robot actions, can generate control signals end-to-end from task descriptions and image observations, showing promise for generalization in open-world environments [2, 27, 28].

However, such monolithic VLA models face several challenges. First, collecting high-quality robot manipulation data is costly, limiting the scale and diversity of available training data [3, 29, 30]. Second, VLA models have not yet achieved the level of classical robot control in terms of operational speed and precise motion control. On the other hand, smaller policy models can achieve more precise control compared to monolithic VLA models, but struggle to generalize to conditions different from their training environments [2, 4]. To address these challenges, hierarchical approaches that separate high-level semantic understanding by VLMs from precise motion generation by smaller policies have been proposed [3, 5, 11].

HAMSTER, one such hierarchical approach, uses a VLM (VILA-1.5-13B) fine-tuned on robot path data to generate 2D end-effector paths, which serve as guidance for low-level policies such as RVT-2 or 3D Diffuser Actor [3]. This design allows the VLM to focus on semantic trajectory planning while the low-level policy concentrates on precise control in 3D space. HAMSTER is also notable for its ability to leverage off-domain data without action labels, such as simulation data or data from different embodiments.

Meanwhile, generative models based on diffusion have demonstrated strong performance for robot manipulation [6, 13, 31]. However, diffusion models require numerous denoising steps during inference, posing challenges for real-time control. ManiFlow addresses this by adopting Consistency Flow Matching, enabling high-quality action generation in just 1-2 steps [7]. ManiFlow employs the DiT-X (Diffusion Transformer with Cross-Attention) architecture to efficiently integrate diverse inputs including visual, language, and proprioceptive information.

In this work, we propose adopting a mechanism based on ManiFlow as the low-level policy within HAMSTER's hierarchical architecture and integrating 2D paths generated by the VLM into ManiFlow's input. While the original ManiFlow was evaluated under conditions identical to training, we investigate whether introducing VILA path guidance can improve (1) single-task accuracy in the same environment, and (2) generalization performance to evaluation environments different from training.

Specifically, we propose two patterns for path input. The first pattern uses only the path prediction for the current frame. The second pattern additionally retains the path prediction from episode onset, using it alongside the current path. We refer to the latter as the Memory Function. This design aims to enable reference to stable past path information when current-frame path generation becomes unstable due to occlusion or other factors.

Evaluation experiments are conducted in the RoboTwin 2.0 simulation environment [8]. We establish six experimental conditions by combining training environments (clean table or cluttered table) with evaluation environments (cluttered table), and quantitatively analyze the effects of path guidance on in-domain accuracy and cross-domain generalization performance.

## 2. Related Work

### 2.1 Hierarchical Robot Manipulation Policies

Hierarchical approaches in robot manipulation aim to solve complex tasks by separating high-level planning from low-level control. SayCan proposed combining the knowledge of large language models (LLMs) with robot affordance functions to generate action plans that are both executable and semantically appropriate [5]. Code as Policies demonstrated a hierarchical code generation approach that uses LLMs to generate program code for robot control, enabling complex spatial reasoning [9]. Inner Monologue achieved closed-loop reasoning by providing environmental feedback to LLMs, integrating success detection and scene descriptions [32].

In hierarchical VLA models using VLMs, the choice of intermediate representation generated by the high-level model is an important design decision. Keypoint-based affordance prediction [10] predicts manipulation target positions and placement locations as points, achieved through VLM fine-tuning. RT-Trajectory used 2D trajectory sketches as an intermediate representation to enable flexible task specification and policy generalization [11]. For low-level policies, 3D-aware policies such as Perceiver-Actor [33] using voxelized 3D observations, and RVT [34] and RVT-2 [35] using multi-view images from virtual viewpoints, have demonstrated high accuracy.

HAMSTER proposed a hierarchical architecture where a VLM predicts 2D end-effector paths, which serve as guidance for low-level 3D-aware policies [3]. A key advantage of this design is that the high-level VLM can be trained on off-domain data without action labels, such as simulation data or data from different embodiments. In HAMSTER, methods of overlaying path information on images and concatenating it as a separate input dimension were compared, with the latter showing superior performance. In this work, we adopt the method of overlaying paths onto images and using them as input to ManiFlow.

### 2.2 Diffusion Models and Flow Matching for Robot Control

Diffusion models are generative models that learn gradual transformations from noise to data, achieving significant success in image generation [12, 36]. Diffusion Policy applied diffusion models to robot visuomotor policies, demonstrating strong performance in representing multimodal action distributions and handling high-dimensional action spaces [6]. 3D Diffusion Policy further improved visual generalization by using 3D representations extracted from sparse point clouds [13]. 3D Diffuser Actor combined 3D scene representations with Diffusion Policy, achieving high performance on the RLBench benchmark [31]. However, diffusion models require numerous denoising steps during inference (typically 10-100 steps), posing challenges for real-time control applications.

Several approaches have been proposed to address this inference efficiency problem. DDIM, as a deterministic sampling variant of DDPM, enabled high-quality generation with fewer steps [37]. Consistency Models enabled single-step generation by learning direct mappings from arbitrary points in the diffusion process to target data [14]. For robot control applications, Consistency Policy proposed accelerating inference through knowledge distillation from pretrained Diffusion Policies [19]. ManiCM applied Consistency Models to 3D Diffusion Policy, achieving real-time 3D manipulation [20].

Flow Matching, on the other hand, constructs generative models through a different approach than diffusion models. Flow Matching directly learns continuous transformations (flows) from noise distributions to data distributions in a simulation-free manner [15]. Rectified Flow achieved more efficient sampling by straightening these flows [21]. These methods offer more stable training and higher-quality generation with fewer steps compared to diffusion models. π₀ combined VLA models with Flow Matching, achieving general control across diverse robot configurations including single-arm, bimanual, and mobile manipulators [22].

ManiFlow adopts Consistency Flow Matching, which integrates Flow Matching and Consistency Training [7]. Specifically, in addition to the standard Flow Matching loss, it simultaneously optimizes a continuous-time Consistency loss that constrains different points on flow trajectories to converge to the same target data point. This design enables high-quality generation in 1-2 steps without requiring pretrained teacher models. The core of ManiFlow's architecture, DiT-X, is built upon Diffusion Transformer [16] and efficiently integrates diverse input modalities including visual, language, and proprioceptive information through cross-attention and AdaLN-Zero conditioning. In this work, we adopt ManiFlow as the low-level policy and integrate VLM-generated paths as additional visual input.

### 2.3 Domain Adaptation and Generalization

The domain gap between training and evaluation environments is a critical challenge when deploying robot manipulation policies to real environments. Domain Randomization, which randomizes rendering parameters in simulation environments, is widely used to facilitate transfer to real environments [17]. Colosseum is a benchmark that evaluates policy generalization through 20 tasks × 14 axes of environmental perturbations, measuring robustness to changes in color, texture, size, lighting, and background [38].

Pretraining visual representations is also an effective approach for improving generalization. R3M pretrained visual representations using large-scale human video data, improving transfer performance to robot manipulation tasks through temporal contrastive learning and video-language alignment [18]. CLIP performed contrastive learning on 400 million image-text pairs, achieving zero-shot transfer capabilities [39]. DINOv2 performed self-supervised learning on 142 million images, providing general visual features without fine-tuning [40].

RoboTwin 2.0 is a large-scale benchmark containing 731 objects, 50 tasks, and 5 robot embodiment types, implementing 5 axes of Domain Randomization including Clutter, Lighting, and Background [8]. Compared to standard benchmarks such as RLBench [41] and MetaWorld [42], it enables evaluation with more realistic visual variations. In this work, we use the RoboTwin 2.0 environment to evaluate how well policies trained on clean table conditions can generalize to cluttered table conditions. Whether VLM path guidance can bridge such domain gaps is one of the main questions investigated in this study.

## 3. Proposed Method

This chapter describes the proposed system that integrates 2D path guidance from VLMs into ManiFlow.

### 3.1 System Overview

The proposed system adopts a hierarchical architecture consisting of a high-level path generation module and a low-level action generation module. Figure 1 shows the overall system configuration.

【Figure 1: System overview - VILA-13B (high-level) on the left, ManiFlow (low-level) on the right. RGB image and task instruction as input at the top, 2D path output from VILA overlaid onto RGB image and fed to ManiFlow. ManiFlow outputs 16 steps of actions. Both initial overlay and current overlay paths are illustrated.】

The high-level module uses VILA-1.5-13B [23]. VILA takes an RGB image and task description as input and outputs the 2D path that the end-effector should follow. The output path is represented as a sequence of normalized 2D coordinates (each coordinate in the range [0, 1]) and gripper states (open/close). We use the model fine-tuned on robot manipulation data from the HAMSTER project [3].

The low-level module uses ManiFlow [7]. The original ManiFlow takes RGB images from the past 2 steps and the robot's proprioceptive state (joint angles and gripper state) as input, generating 16 steps of actions at once. In this work, we overlay the 2D path generated by VILA onto these input images, thereby conveying high-level trajectory guidance to the low-level policy.

The proposed system provides two patterns for path input:

**Pattern 1: Current path only**

Only the path generated by VILA for the current frame is used. Specifically, the current path is overlaid onto each of the two past observation images that serve as input to ManiFlow, and provided to ManiFlow along with the robot state.

**Pattern 2: Initial path + Current path (Memory Function)**

Both the path generated at episode onset (initial path) and the path generated for the current frame (current path) are used. This design aims to enable reference to stable initial path information when current-frame path generation becomes unstable due to occlusion during task execution. We refer to this mechanism as the Memory Function.

The inference flow proceeds as follows. Since ManiFlow predicts 16 steps of actions at once, path generation occurs every 16 steps rather than every frame:

1. **Step 0**: Execute VILA path generation and create initial overlay and current overlay. Input these to ManiFlow and predict 16 steps of actions at once.
2. **Steps 1-15**: Sequentially execute the predicted actions.
3. **Step 16**: Execute new VILA path generation and update the current overlay. The initial overlay remains fixed from episode onset. Predict the next 16 steps of actions at once.
4. Repeat every 16 steps until task completion.

This design limits the overhead of VILA path generation to once every 16 steps, minimizing the impact on real-time control.

### 3.2 High-Level Path Generation

The high-level module uses VILA-1.5-13B [23], which was fine-tuned on robot manipulation data in the HAMSTER project [3]. VILA is a vision-language model combining the SigLIP visual encoder with the Vicuna language model, pretrained on 5.3 billion image-text pairs.

#### Input and Output

The input to VILA consists of an RGB image and a natural language task instruction. Task instructions are given in forms such as "there is a hammer and a block on the table, use the arm to grab the hammer and beat the block" or "click the bell's top center on the table."

VILA's output is structured in the following format:

```
<ans>[(x1, y1), (x2, y2), <action>Close Gripper</action>, (x3, y3), ...]</ans>
```

Here, each coordinate (x, y) represents a 2D position normalized to the range [0, 1], and `<action>` tags indicate gripper state change points. By parsing this output, we obtain a path representation including the gripper state (open/close) corresponding to each waypoint.

#### Path Visualization

Generated paths are rendered as overlays on the original RGB image. The rendering specifications follow the original HAMSTER [3] implementation:

- **Path color**: Uses the jet colormap, encoding temporal progression through color changes from blue → cyan → green → yellow → red
- **Line width**: Scaled according to image size (based on 512×512 images)
- **Gripper markers**: Circular markers drawn only at gripper state change points
  - Open state: Blue circle (BGR: 255, 0, 0)
  - Close state: Red circle (BGR: 0, 0, 255)

Figure 2 shows examples of path overlays.

【Figure 2: Path overlay examples - For three tasks (click_bell, move_can_pot, beat_block_hammer), overlay images for both clean and cluttered conditions are displayed side by side. Examples showing path color changes via jet colormap and markers at gripper state change points.】

#### Path Generation Reliability

Since VILA is configured with temperature=0, the output is deterministic, but some frames may produce outputs that do not follow the specified format. To address this issue, we adopt the following strategies:

1. **Retry mechanism**: Up to 2 path generation attempts per frame
2. **Fallback parsing**: Automatic completion for outputs interpretable as coordinate data even when `<ans>` tags are omitted or coordinate parentheses are missing
3. **Fallback path**: When path generation fails, the most recently successful path is used as a substitute. For consecutive failures at episode onset, the first successful path within the episode is retroactively applied

This design ensures the same fallback strategy is applied during both training and evaluation, maintaining consistency in the input distribution.

### 3.3 Low-Level Action Generation

The low-level module uses ManiFlow [7], a robot manipulation policy based on Consistency Flow Matching. This section describes ManiFlow's architecture and how path input is integrated in this work.

#### ManiFlow Overview

ManiFlow achieves high-quality action generation in 1-2 steps through Consistency Flow Matching, which integrates Flow Matching [15] and Consistency Training [14]. While conventional Diffusion Policy [6] requires 10-100 denoising steps during inference, ManiFlow achieves comparable or better quality with significantly fewer steps.

ManiFlow training simultaneously optimizes the standard Flow Matching loss and Consistency loss. The Flow Matching loss learns the velocity field from noise points to data points:

$$\mathcal{L}_{FM}(\theta) = \mathbb{E}_{x_0, x_1}[\|v_\theta(x_t, t) - (x_1 - x_0)\|^2]$$

Here, $x_t = (1-t)x_0 + tx_1$ is the linear interpolation between noise $x_0$ and data $x_1$. The Consistency loss constrains different points on flow trajectories to converge to the same target:

$$\mathcal{L}_{CT}(\theta) = \mathbb{E}_{t, \Delta t}[\|v_\theta(x_t, t, \Delta t) - \tilde{v}_{target}\|^2]$$

By combining these losses, high-quality generation is achieved in few steps without requiring pretrained teacher models.

#### DiT-X Architecture

The core of ManiFlow's architecture is DiT-X (Diffusion Transformer with Cross-Attention). DiT-X is built upon Diffusion Transformer [16] and features:

- **Adaptive cross-attention**: Enables fine-grained token-level interactions between action tokens and observation features (visual, language)
- **AdaLN-Zero conditioning**: Dynamically generates scale and shift parameters based on low-dimensional inputs such as timesteps and robot states, adaptively adjusting network behavior

These mechanisms enable efficient multimodal integration of visual, language, and proprioceptive inputs.

#### Path Input Integration in This Work

The original ManiFlow takes RGB images from the past 2 steps and the robot's proprioceptive state (14 dimensions: 7 DoF joint positions and gripper state for each arm) as input, generating 16 steps of actions at once.

In this work, we integrate high-level guidance from VILA by replacing the input RGB images with path overlay images. Table 1 shows the specific input configuration for each condition.

**Table 1: Input Configuration for Each Condition**

| Condition | Input Image | Proprioception | Output |
|-----------|-------------|----------------|--------|
| Original ManiFlow | RGB image (past 2 steps) | 14 dim | 16 steps of actions |
| Current path only | current overlay (past 2 steps) | 14 dim | 16 steps of actions |
| Initial + Current | initial overlay + current overlay | 14 dim | 16 steps of actions |

Path overlay images are the original RGB images with the 2D path generated by VILA visually rendered on them. ManiFlow is expected to learn the overlaid path information as visual features and utilize it for task execution.

### 3.4 Memory Function

In this work, we propose a second pattern for path input that combines the path from episode onset (initial path) with the current frame path (current path). We refer to this mechanism as the Memory Function.

During task execution, occlusion may occur when the robot's end-effector or grasped objects obstruct the field of view. In such situations, the quality of paths generated by VILA may degrade, producing inaccurate or incomplete trajectories. At episode onset (frame 0), occlusion typically does not occur, and the task target objects are clearly visible. Therefore, paths generated at this point are considered reliable as overall trajectory plans for the task. The Memory Function maintains this initial path, enabling reference to overall task context information even when current path quality degrades during task execution.

When using the Memory Function, input to ManiFlow is configured as follows:

- **Initial overlay**: RGB image at episode onset (frame 0) with the path generated at that time overlaid. Fixed until episode end.
- **Current overlay**: Current frame RGB image with the path generated for that frame overlaid. Updated every 16 steps.
- **Proprioceptive state**: Current robot state (14 dimensions)

Figure 3 shows a conceptual diagram of the Memory Function.

【Figure 3: Memory Function conceptual diagram - Along the time axis, illustrating how the initial overlay is generated and fixed at frame 0, and how the current overlay is updated at frames 16, 32, etc. Visually expressing that even when current path becomes unstable due to occlusion, the initial path remains available for reference.】

This mechanism is expected to improve occlusion robustness, preserve task context, and enable utilization of long-term planning information. Compared to using only the current path (Pattern 1), this approach incurs slightly higher computational cost due to the additional input image, but offers the advantage of maintaining the overall plan from the initial state. In our experiments, we compare these two patterns and quantitatively verify the effectiveness of the Memory Function.

## 4. Experiments

This chapter describes the experimental setup and results for verifying the effectiveness of the proposed method.

### 4.1 Experimental Setup

Experiments were conducted in the RoboTwin 2.0 simulation environment [8]. RoboTwin 2.0 is built on the SAPIEN 3.0.0b1 physics engine [24] and provides benchmarks for bimanual manipulation tasks using the AgileX Cobot Magic robot. This work focuses on single-arm tasks, using the clean table and cluttered table conditions provided as table environment settings. In the cluttered table condition, objects unrelated to the task are randomly placed on the table, increasing visual complexity.

For evaluation tasks, we selected 3 tasks from RoboTwin 2.0's single-arm tasks, considering difficulty and diversity of motion patterns. Click_bell involves simple contact motion, move_can_pot is a standard pick-and-place task, and beat_block_hammer requires tool use with two-stage motion. For each task, we collected 50 successful trajectories using RoboTwin 2.0's scripted expert policy as training data.

In this work, we established 6 experimental conditions (C1-C6) by combining 3 model configurations (original ManiFlow, VILA + ManiFlow (current path), VILA + ManiFlow (initial + current path)) with 2 training environments (cluttered, clean). Table 2 shows the details of each condition. All conditions were evaluated on the cluttered table condition. Conditions C1-C3 are in-domain settings with training and evaluation on cluttered environments, while conditions C4-C6 are cross-domain settings with training on clean environments and evaluation on cluttered environments.

**Table 2: Experimental Conditions**

| Condition | Training Env | Evaluation Env | Path Input |
|-----------|--------------|----------------|------------|
| C1 | Cluttered | Cluttered | None (Original ManiFlow) |
| C2 | Cluttered | Cluttered | Current path |
| C3 | Cluttered | Cluttered | Initial + Current path |
| C4 | Clean | Cluttered | None (Original ManiFlow) |
| C5 | Clean | Cluttered | Current path |
| C6 | Clean | Cluttered | Initial + Current path |

This design tests the following four hypotheses: **H1** (VILA path guidance improves in-domain accuracy), **H2** (VILA path guidance improves cross-domain generalization), **H3** (Memory Function outperforms current path only), and **H4** (path guidance effects are more pronounced in cross-domain conditions).

For evaluation metrics, we used task success rate based on 100 trials per task as the primary metric, with success determined using RoboTwin 2.0's built-in judgment engine. As auxiliary metrics, we measured VILA path generation time, ManiFlow action generation time, and path generation success rate. ManiFlow training settings were unified across all conditions: 2 observation steps, 16 action prediction steps, and 501 training epochs.

### 4.2 Results and Analysis

#### 4.2.1 Effect of Input Representation Mode

Figure 4 shows success rates by condition, and Table 3 provides detailed numerical results. This section verifies hypotheses H1-H4 regarding the effects of path input modes.

【Figure 4: Success rate by condition - Use `/gscratch/scrubbed/naoto03/projects/HAMSTER-ManiFlow-Integration/analysis/outputs/figures/main_results_all.png`】

**Table 3: Task Success Rate (%)**

| Task | C1 | C2 | C3 | C4 | C5 | C6 |
|:-----|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| Click Bell | 4.0 | 2.0 | 4.0 | 3.0 | **20.0** | 1.0 |
| Move Can to Pot | 21.0 | 20.0 | 26.0 | 27.0 | 27.0 | 27.0 |
| Beat Block with Hammer | 4.0 | 6.0 | 1.0 | 31.0 | **37.0** | 11.0 |
| **Average** | 9.7 | 9.3 | 10.3 | 20.3 | **28.0** | 13.0 |

**H1: Path Guidance Effect in In-domain Conditions**

We compared C1 (baseline) with C2 and C3 (with path). C1→C2 showed -0.3% on average, and C1→C3 showed +0.7% on average, indicating no consistent improvement from VILA path guidance. This result suggests that path guidance may not provide additional information in in-domain conditions. Furthermore, the visual changes introduced by path overlays may have increased visual noise in cluttered training environments, making learning more difficult for ManiFlow.

**H2: Path Guidance Effect in Cross-domain Conditions**

We compared C4 (baseline) with C5 and C6 (with path). C4→C5 showed an average improvement of +7.7%, with improvements confirmed in 2 of 3 tasks (Click Bell: +17.0%, Beat Block with Hammer: +6.0%). In contrast, C4→C6 showed a -7.3% decrease in performance. The partial effectiveness of path guidance in cross-domain conditions can be attributed to VILA's paths compensating for the visual differences between training (clean) and evaluation (cluttered) environments. Since VILA is pretrained on large-scale data, it can generate appropriate paths even in cluttered environments, providing ManiFlow with trajectory information necessary for task execution.

**H3: Effect of Memory Function (Initial + Current path)**

We compared C2 vs C3 and C5 vs C6. In in-domain conditions (C2→C3), there was a slight improvement of +1.0% on average, but in cross-domain conditions (C5→C6), a substantial performance decrease of -15.0% on average was observed. This contrasting result indicates that the effectiveness of initial path strongly depends on domain conditions. In in-domain conditions, the initial path may have functioned somewhat as contextual information for the overall task, contributing slightly to performance improvement. In cross-domain conditions, however, inputting two overlay images may have increased the amount of information to process, becoming a burden for models trained on clean environments. Additionally, as the episode progressed, information from the initial path (at frame 0) may have diverged from the current situation, providing misleading guidance. This result suggests that the effectiveness of Memory Function depends on the combination of training and evaluation environments.

**H4: Domain Dependency of Path Guidance Effect**

We compared (C2-C1) vs (C5-C4) and (C3-C1) vs (C6-C4). For the current path mode, the cross-domain improvement (+7.7%) was larger than the in-domain improvement (-0.3%), and this trend was confirmed in 2 of 3 tasks. In contrast, the initial+current mode showed the opposite pattern, with in-domain improvement (+0.7%) versus a substantial cross-domain decrease (-7.3%). This contrasting result indicates that the adaptation characteristics to domain gaps differ depending on the path input mode. The current path mode demonstrates effectiveness in cross-domain conditions where VILA's semantic guidance bridges the domain gap, while the initial+current mode is only effective in in-domain conditions and becomes counterproductive when domain gaps exist due to information inconsistency.

#### 4.2.2 Effect of Training Environment

From the results in Table 3, an unexpected finding emerged that was not hypothesized. Conditions trained on clean environments (C4-C6, average 13.0-28.0%) consistently showed higher success rates than conditions trained on cluttered environments (C1-C3, average 9.3-10.3%). Particularly for the Beat Block with Hammer task, C4 (31.0%) and C5 (37.0%) substantially outperformed C1-C3 (1.0-6.0%).

This result suggests that models trained on cluttered environments may have overfit to the scatter patterns specific to training data and failed to handle different scatter patterns during evaluation. In contrast, models trained on clean environments could efficiently learn essential motion patterns for tasks in environments with less visual noise, resulting in improved generalization to unseen cluttered environments.

#### 4.2.3 Task-Specific Analysis

Different result patterns were observed across tasks. For Click Bell, C5 showed an outstanding success rate (20.0%), while other conditions remained low at 1.0-4.0%. For Move Can to Pot, differences between conditions were relatively small (20.0-27.0%), indicating limited effectiveness of path guidance. For Beat Block with Hammer, high success rates were achieved in C4 (31.0%) and C5 (37.0%), but other conditions remained low at 1.0-11.0%.

These results suggest that the effectiveness of VILA path guidance depends on task characteristics. For tasks requiring precise manipulation or clear trajectory planning (Click Bell, Beat Block with Hammer), appropriately generated paths contribute to improved motion accuracy. In contrast, for standard pick-and-place tasks like Move Can to Pot, motion patterns are relatively simple, limiting additional improvement from path guidance.

#### 4.2.4 Training Dynamics

We conducted 501 epochs of training for all 18 conditions (3 tasks × 6 conditions) and analyzed training curves (Figure 5). Table 4 shows training statistics.

【Figure 5: Training curve details - Use `/gscratch/scrubbed/naoto03/projects/HAMSTER-ManiFlow-Integration/analysis/outputs/figures/training_curves_by_task.png`】

**Table 4: Training Statistics**

| Task | Condition | Initial Loss | Final Loss | Min Loss |
|:-----|:---------:|:------------:|:----------:|:--------:|
| Click Bell | C1-C3 | 2.76-2.77 | 0.003-0.003 | 0.003-0.003 |
| | C4-C6 | 2.76-2.77 | 0.003-0.003 | 0.002-0.003 |
| Move Can to Pot | C1-C3 | 2.33-2.34 | 0.002-0.002 | 0.002-0.002 |
| | C4-C6 | 2.33-2.34 | 0.002-0.003 | 0.002-0.002 |
| Beat Block with Hammer | C1-C3 | 2.51-2.53 | 0.001-0.004 | 0.0003-0.001 |
| | C4-C6 | 2.25-2.28 | 0.002-0.002 | 0.001-0.001 |

All conditions achieved 99.9% loss reduction, and training converged normally. Differences in initial loss values were observed across tasks, with Click Bell (approximately 2.76) > Beat Block with Hammer (approximately 2.25-2.53) > Move Can to Pot (approximately 2.33). For Beat Block with Hammer, clean environment conditions had approximately 10% lower initial loss than cluttered environment conditions, suggesting that data from clean environments has representations that are easier to learn.

#### 4.2.5 Computational Cost

**Table 5: Inference Time (ms)**

| Condition | VILA | ManiFlow | Total Overhead |
|:---------:|:----:|:--------:|:--------------:|
| C1, C4 | - | 108-136 | - |
| C2, C5 | 4313-4703 | 118-136 | +4.3-4.7s |
| C3, C6 | 4361-4363 | 118-126 | +4.4s |

Path generation by VILA incurs an overhead of approximately 4.3-4.7 seconds. Since ManiFlow alone has fast inference time of approximately 100-140ms, with the current design of VILA inference every 16 steps, overall episode execution time is approximately doubled. Considering C5's improvement (+7.7%) in cross-domain conditions, this overhead may be acceptable in application scenarios where generalization performance is important. For applications requiring real-time performance, lighter VLMs [reference needed: literature on lightweight VLMs such as NVILA] or optimization of path generation frequency is necessary.

### 4.3 Limitations

This study has several limitations.

First, evaluations in this study were conducted only in the RoboTwin 2.0 simulation environment, without verification on real robots. A sim-to-real gap exists between simulation and real environments, and differences in visual appearance, physical properties, and sensor noise may affect performance. The extent to which VILA path guidance is robust to this sim-to-real gap needs to be verified through future real robot experiments.

Second, the tasks used for evaluation were limited to 3 types (click_bell, move_can_pot, beat_block_hammer). While these tasks were selected considering difficulty and diversity of motion patterns, the effects of path guidance on more complex long-horizon tasks, bimanual coordination tasks, or contact-rich manipulation tasks remain unverified. Given that the effectiveness of path guidance was suggested to depend on task characteristics, evaluation on a broader set of tasks is necessary.

Third, regarding the unexpected performance degradation caused by the Memory Function (initial + current path), this study did not sufficiently elucidate its causes. The original design intent was to improve robustness by referencing the initial path when occlusion occurs. However, we did not quantitatively evaluate how much occlusion occurred in the tasks used in this experiment, nor how much it affected path generation quality. Future work should quantitatively analyze the frequency and impact of occlusion, and design more sophisticated Memory mechanisms such as adaptive path reference based on occlusion detection.

## 5. Conclusion

In this work, we proposed a method that adopts ManiFlow as the low-level policy in HAMSTER's hierarchical architecture and integrates 2D paths generated by VLMs into ManiFlow's input. Through systematic experiments with 3 tasks × 6 conditions in the RoboTwin 2.0 simulation environment, we verified the effects of path guidance on single-task accuracy and generalization performance.

The experiments yielded the following findings. First, models trained on clean environments consistently showed higher generalization performance than models trained on cluttered environments (average success rate 20.3-28.0% vs 9.3-10.3%). This suggests that learning in visually simple environments promotes acquisition of essential motion patterns for tasks, consequently improving generalization to unseen environments. Second, VILA path guidance was effective in cross-domain conditions (clean training → cluttered evaluation), achieving an average +7.7% improvement in current path mode. This indicates the potential for VLM semantic guidance to bridge domain gaps. Third, the Memory Function (initial + current path) unexpectedly caused performance degradation, revealing that additional information is not necessarily beneficial in input representation design.

This study has clarified the potential and challenges of hierarchical approaches combining high-level trajectory planning by VLMs with fast action generation by Consistency Flow Matching. Future work should further verify the practicality of the proposed method through real robot verification, evaluation on a broader set of tasks, and design of Memory mechanisms adaptive to occlusion.

## Acknowledgments

This work was supported by Cross-Pacific AI Initiative (X-PAI).

## References

[1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control," in *Proc. Conference on Robot Learning (CoRL)*, 2023.

[2] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn, "OpenVLA: An Open-Source Vision-Language-Action Model," in *Proc. Conference on Robot Learning (CoRL)*, 2024.

[3] Y. Li, Y. Deng, J. Zhang, J. Jang, M. Memmel, R. Yu, C. Garrett, F. Ramos, D. Fox, A. Li, A. Gupta, and A. Goyal, "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation," *arXiv preprint arXiv:2502.05485*, 2025.

[4] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, "RT-1: Robotics Transformer for Real-World Control at Scale," *arXiv preprint arXiv:2212.06817*, 2022.

[5] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reber, C. Samiento, N. Siebers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng, "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances," *arXiv preprint arXiv:2204.01691*, 2022.

[6] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song, "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion," in *Proc. Robotics: Science and Systems (RSS)*, 2023.

[7] G. Yan, J. Zhu, Y. Deng, S. Yang, R. Qiu, X. Cheng, M. Memmel, R. Krishna, A. Goyal, X. Wang, and D. Fox, "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training," *arXiv preprint arXiv:2509.01819*, 2025.

[8] T. Chen, J. Wang, Y. Mu, Z. Liu, Y. Yuan, Y. Zhang, C. Wang, R. Qiu, P. Lu, and H. Dong, "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation," *arXiv preprint arXiv:2506.18088*, 2025.

[9] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as Policies: Language Model Programs for Embodied Control," in *Proc. IEEE International Conference on Robotics and Automation (ICRA)*, 2023.

[10] W. Yuan, T. Ren, M. Memmel, D. Fox, A. Gupta, and A. Goyal, "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics," in *Proc. Conference on Robot Learning (CoRL)*, 2024.

[11] H. Gu, Y. Su, Y. Liu, S. Jiang, K. Pertsch, J. Luo, A. Mandlekar, D. Xu, L. Fei-Fei, and J. Wu, "RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches," *arXiv preprint arXiv:2311.01977*, 2023.

[12] J. Ho, A. Jain, and P. Abbeel, "Denoising Diffusion Probabilistic Models," in *Proc. Advances in Neural Information Processing Systems (NeurIPS)*, 2020.

[13] Y. Ze, G. Zhang, K. Zhang, C. Hu, J. Wang, and H. Dong, "3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations," in *Proc. Robotics: Science and Systems (RSS)*, 2024.

[14] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, "Consistency Models," in *Proc. International Conference on Machine Learning (ICML)*, 2023.

[15] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, "Flow Matching for Generative Modeling," in *Proc. International Conference on Learning Representations (ICLR)*, 2023.

[16] W. Peebles and S. Xie, "Scalable Diffusion Models with Transformers," in *Proc. IEEE/CVF International Conference on Computer Vision (ICCV)*, 2023.

[17] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World," in *Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2017.

[18] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, "R3M: A Universal Visual Representation for Robot Manipulation," in *Proc. Conference on Robot Learning (CoRL)*, 2022.

[19] P. Prasad, R. Hoque, S. Tung, I. Pinto, K. Kawaguchi, and Y. Shkurti, "Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation," in *Proc. Robotics: Science and Systems (RSS)*, 2024.

[20] Y. Lu, H. Chen, Y. Huang, Z. Chen, X. Cheng, and H. Wang, "ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation," *arXiv preprint arXiv:2406.01586*, 2024.

[21] X. Liu, C. Gong, and Q. Liu, "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow," in *Proc. International Conference on Learning Representations (ICLR)*, 2023.

[22] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, S. Jakber, T. Kelestemur, S. Levine, A. Lii, I. Liang, H. Luo, S. Nair, K. Pertsch, L. Qi, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, K. Tran, T. Vuong, F. Xia, Z. Xu, T. Xiao, H. Xu, M. Xu, and S. Yeola, "π₀: A Vision-Language-Action Flow Model for General Robot Control," *arXiv preprint arXiv:2410.24164*, 2024.

[23] J. Lin, H. Yin, W. Ping, P. Molchanov, M. Shoeybi, and S. Han, "VILA: On Pre-training for Visual Language Models," in *Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2024.

[24] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su, "SAPIEN: A SimulAted Part-based Interactive ENvironment," in *Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.

[25] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Tober, G. Welker, P. Wohlhart, J. Wu, and P. R. Florence, "PaLM-E: An Embodied Multimodal Language Model," in *Proc. International Conference on Machine Learning (ICML)*, 2023.

[26] D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, J. Luo, Y. L. Tan, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine, "Octo: An Open-Source Generalist Robot Policy," in *Proc. Robotics: Science and Systems (RSS)*, 2024.

[27] Open X-Embodiment Collaboration, "Open X-Embodiment: Robotic Learning Datasets and RT-X Models," in *Proc. IEEE International Conference on Robotics and Automation (ICRA)*, 2024.

[28] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu, H. Li, and P. Luo, "Vision-Language Foundation Models as Effective Robot Imitators," *arXiv preprint arXiv:2311.01378*, 2023.

[29] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. David, K. Black, A. Kumar, C. Xu, Q. Vuong, and C. Finn, "DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset," in *Proc. Robotics: Science and Systems (RSS)*, 2024.

[30] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine, "BridgeData V2: A Dataset for Robot Learning at Scale," in *Proc. Conference on Robot Learning (CoRL)*, 2023.

[31] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations," in *Proc. Conference on Robot Learning (CoRL)*, 2024.

[32] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, "Inner Monologue: Embodied Reasoning through Planning with Language Models," in *Proc. Conference on Robot Learning (CoRL)*, 2022.

[33] M. Shridhar, L. Manuelli, and D. Fox, "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation," in *Proc. Conference on Robot Learning (CoRL)*, 2022.

[34] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox, "RVT: Robotic View Transformer for 3D Object Manipulation," in *Proc. Conference on Robot Learning (CoRL)*, 2023.

[35] A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, and D. Fox, "RVT-2: Learning Precise Manipulation from Few Demonstrations," in *Proc. Robotics: Science and Systems (RSS)*, 2024.

[36] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, "Score-Based Generative Modeling through Stochastic Differential Equations," in *Proc. International Conference on Learning Representations (ICLR)*, 2021.

[37] J. Song, C. Meng, and S. Ermon, "Denoising Diffusion Implicit Models," in *Proc. International Conference on Learning Representations (ICLR)*, 2021.

[38] A. Pumacay, I. Singh, J. Duan, F. Xia, J. Thomason, and D. Fox, "THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation," in *Proc. Robotics: Science and Systems (RSS)*, 2024.

[39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning Transferable Visual Models From Natural Language Supervision," in *Proc. International Conference on Machine Learning (ICML)*, 2021.

[40] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jégou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, "DINOv2: Learning Robust Visual Features without Supervision," *arXiv preprint arXiv:2304.07193*, 2023.

[41] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, "RLBench: The Robot Learning Benchmark," *IEEE Robotics and Automation Letters*, vol. 5, no. 2, pp. 3019-3026, 2020.

[42] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning," in *Proc. Conference on Robot Learning (CoRL)*, 2020.
